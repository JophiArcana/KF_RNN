{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c446f8-1a49-4903-893d-b7a6cc40cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from argparse import Namespace\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "import tensordict.utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "from tensordict import TensorDict\n",
    "from transformers import GPT2Config\n",
    "\n",
    "# This line needs to be added since some terminals will not recognize the current directory\n",
    "PROJECT_NAME: str = \"KF_RNN\"\n",
    "PROJECT_PATH: str = os.getcwd()[:os.getcwd().find(PROJECT_NAME) + len(PROJECT_NAME)]\n",
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "from infrastructure import loader\n",
    "from infrastructure import utils\n",
    "from infrastructure.experiment import *\n",
    "from infrastructure.settings import DEVICE\n",
    "from infrastructure.utils import PTR\n",
    "from model.convolutional import CnnKFLeastSquares\n",
    "from model.kf import KF\n",
    "from model.sequential import RnnKFPretrainAnalytical\n",
    "from model.transformer import GPT2InContextKF\n",
    "from model.zero_predictor import ZeroPredictor\n",
    "from system.linear_time_invariant import LinearSystemGroup, MOPDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60d872-b537-41f2-8574-d79320957465",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"in_context\"\n",
    "output_fname = \"result\"\n",
    "\n",
    "SHP = Namespace(S_D=10, I_D=1, O_D=5, input_enabled=False)\n",
    "\n",
    "context_length = 250\n",
    "n_train_systems = 40000\n",
    "n_test_systems = 3\n",
    "valid_dataset_size = 2000\n",
    "test_dataset_size = 256\n",
    "\n",
    "n_firs = 5\n",
    "rnn_increment = 5\n",
    "\n",
    "save_file = \"sandbox/cdc_reconstruction_save.pt\"\n",
    "if os.path.exists(save_file):\n",
    "    save = torch.load(save_file, map_location=DEVICE)\n",
    "\n",
    "    systems, dataset, result_transformer, result_baseline_cnn, result_baseline_rnn = map(vars(save).__getitem__, (\n",
    "        \"systems\",\n",
    "        \"dataset\",\n",
    "        \"result_transformer\",\n",
    "        \"result_baseline_cnn\",\n",
    "        \"result_baseline_rnn\"\n",
    "    ))\n",
    "else:\n",
    "    \"\"\" Transformer experiment \"\"\"\n",
    "    exp_name_transformer = \"CDCReconstruction_transformer\"\n",
    "\n",
    "    ARGS_TRANSFORMER = loader.generate_args(SHP)\n",
    "    ARGS_TRANSFORMER.model.model = GPT2InContextKF\n",
    "    ARGS_TRANSFORMER.model.gpt2 = GPT2Config(\n",
    "        n_positions=context_length,\n",
    "        n_embd=256,\n",
    "        n_layer=12,\n",
    "        n_head=8,\n",
    "        resid_pdrop=0.0,\n",
    "        embd_pdrop=0.0,\n",
    "        attn_pdrop=0.0,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    ARGS_TRANSFORMER.dataset.train = Namespace(\n",
    "        dataset_size=1,\n",
    "        total_sequence_length=context_length,\n",
    "        system=Namespace(\n",
    "            n_systems=n_train_systems,\n",
    "            distribution=MOPDistribution(\"gaussian\", \"gaussian\", 0.1, 0.1)\n",
    "        )\n",
    "    )\n",
    "    ARGS_TRANSFORMER.dataset.valid = Namespace(\n",
    "        dataset_size=valid_dataset_size,\n",
    "        total_sequence_length=valid_dataset_size * context_length,\n",
    "        system=Namespace(\n",
    "            n_systems=n_test_systems\n",
    "        )\n",
    "    )\n",
    "    ARGS_TRANSFORMER.dataset.test = Namespace(\n",
    "        dataset_size=test_dataset_size,\n",
    "        total_sequence_length=test_dataset_size * context_length,\n",
    "        system=Namespace(\n",
    "            n_systems=n_test_systems\n",
    "        )\n",
    "    )\n",
    "\n",
    "    del ARGS_TRANSFORMER.train.warmup_duration\n",
    "    ARGS_TRANSFORMER.train.epochs = 40000\n",
    "    ARGS_TRANSFORMER.train.subsequence_length = context_length\n",
    "    ARGS_TRANSFORMER.train.batch_size = 28\n",
    "    ARGS_TRANSFORMER.train.iterations_per_epoch = 1\n",
    "\n",
    "    ARGS_TRANSFORMER.train.optim_type = \"Adam\"\n",
    "    ARGS_TRANSFORMER.train.max_lr = 3e-4\n",
    "    ARGS_TRANSFORMER.train.lr_decay = 1.0\n",
    "    ARGS_TRANSFORMER.train.weight_decay = 1e-2\n",
    "\n",
    "    ARGS_TRANSFORMER.experiment.n_experiments = 1\n",
    "    ARGS_TRANSFORMER.experiment.ensemble_size = 1\n",
    "    ARGS_TRANSFORMER.experiment.exp_name = exp_name_transformer\n",
    "    ARGS_TRANSFORMER.experiment.metrics = set() # {\"validation\"}\n",
    "\n",
    "    configurations_transformer = []\n",
    "\n",
    "    result_transformer = run_experiments(\n",
    "        ARGS_TRANSFORMER, configurations_transformer, {\n",
    "            \"dir\": output_dir,\n",
    "            \"fname\": output_fname\n",
    "        }, save_experiment=True\n",
    "    )\n",
    "\n",
    "    systems = utils.multi_map(\n",
    "        lambda lsg: LinearSystemGroup(lsg.state_dict(), SHP.input_enabled),\n",
    "        torch.load(f\"output/{output_dir}/{exp_name_transformer}/testing/systems.pt\", map_location=DEVICE)[\"test\"], dtype=LinearSystemGroup\n",
    "    )\n",
    "    dataset = torch.load(f\"output/{output_dir}/{exp_name_transformer}/testing/dataset.pt\", map_location=DEVICE)[\"test\"]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" Baseline experiment setup \"\"\"\n",
    "    exp_name_baseline_cnn = \"CDCReconstruction_baseline_cnn\"\n",
    "    exp_name_baseline_rnn = \"CDCReconstruction_baseline_rnn\"\n",
    "\n",
    "    for _exp_name_baseline in (exp_name_baseline_cnn, exp_name_baseline_rnn):\n",
    "        os.makedirs(f\"output/{output_dir}/{_exp_name_baseline}/training\", exist_ok=True)\n",
    "        os.makedirs(f\"output/{output_dir}/{_exp_name_baseline}/testing\", exist_ok=True)\n",
    "\n",
    "\n",
    "        if not all(map(os.path.exists, (\n",
    "            f\"output/{output_dir}/{_exp_name_baseline}/training/systems.pt\",\n",
    "            f\"output/{output_dir}/{_exp_name_baseline}/testing/systems.pt\"\n",
    "        ))):\n",
    "            baseline_systems = utils.multi_map(lambda lsg: LinearSystemGroup(lsg.td().permute(1, 0), SHP.input_enabled), systems, dtype=LinearSystemGroup)\n",
    "            torch.save({\n",
    "                \"train\": baseline_systems\n",
    "            }, f\"output/{output_dir}/{_exp_name_baseline}/training/systems.pt\")\n",
    "            torch.save({\n",
    "                \"test\": baseline_systems\n",
    "            }, f\"output/{output_dir}/{_exp_name_baseline}/testing/systems.pt\")\n",
    "\n",
    "\n",
    "        if not all(map(os.path.exists, (\n",
    "            f\"output/{output_dir}/{_exp_name_baseline}/training/dataset.pt\",\n",
    "            f\"output/{output_dir}/{_exp_name_baseline}/testing/dataset.pt\"\n",
    "        ))):\n",
    "            baseline_dataset = utils.multi_map(lambda dataset_: PTR(dataset_.obj.permute(2, 3, 0, 1, 4)), dataset, dtype=PTR)\n",
    "            torch.save({\n",
    "                \"train\": baseline_dataset,\n",
    "                \"valid\": baseline_dataset\n",
    "            }, f\"output/{output_dir}/{_exp_name_baseline}/training/dataset.pt\")\n",
    "            torch.save({\n",
    "                \"test\": baseline_dataset,\n",
    "            }, f\"output/{output_dir}/{_exp_name_baseline}/testing/dataset.pt\")\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" CNN Experiment \"\"\"\n",
    "    ARGS_BASELINE_CNN = loader.generate_args(SHP)\n",
    "    ARGS_BASELINE_CNN.dataset.train = Namespace(\n",
    "        dataset_size=1,\n",
    "        system=Namespace(\n",
    "            n_systems=1,\n",
    "            distribution=MOPDistribution(\"gaussian\", \"gaussian\", 0.1, 0.1)\n",
    "        )\n",
    "    )\n",
    "    ARGS_BASELINE_CNN.dataset.valid = ARGS_BASELINE_CNN.dataset.test = Namespace(\n",
    "        total_sequence_length=context_length\n",
    "    )\n",
    "    ARGS_BASELINE_CNN.experiment.n_experiments = n_test_systems\n",
    "    ARGS_BASELINE_CNN.experiment.ensemble_size = test_dataset_size\n",
    "    ARGS_BASELINE_CNN.experiment.metrics = {\"validation_analytical\"}\n",
    "\n",
    "    # SECTION: Make a copy for RNN args after setting shared parameters\n",
    "    ARGS_BASELINE_RNN = copy.deepcopy(ARGS_BASELINE_CNN)\n",
    "\n",
    "    # SECTION: Set CNN exclusive hyperparameters\n",
    "    ARGS_BASELINE_CNN.model.model = CnnKFLeastSquares\n",
    "    ARGS_BASELINE_CNN.model.ridge = 1.0\n",
    "    ARGS_BASELINE_CNN.experiment.exp_name = exp_name_baseline_cnn\n",
    "\n",
    "    configurations_baseline_cnn = [\n",
    "        (\"model\", {\n",
    "            \"model.ir_length\": [*range(1, n_firs + 1)],\n",
    "        }),\n",
    "        (\"total_trace_length\", {\n",
    "            \"model.model\": [ZeroPredictor] + [CnnKFLeastSquares] * (context_length - 1),\n",
    "            \"dataset.train.total_sequence_length\": [*range(context_length),]\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    result_baseline_cnn = run_experiments(\n",
    "        ARGS_BASELINE_CNN, configurations_baseline_cnn, {\n",
    "            \"dir\": output_dir,\n",
    "            \"fname\": output_fname\n",
    "        }, save_experiment=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" RNN Experiment \"\"\"\n",
    "    # SECTION: Set RNN exclusive hyperparameters\n",
    "    ARGS_BASELINE_RNN.model.model = RnnKFPretrainAnalytical\n",
    "    ARGS_BASELINE_RNN.model.S_D = SHP.S_D\n",
    "    ARGS_BASELINE_RNN.train.optim_type = \"GD\"\n",
    "    ARGS_BASELINE_RNN.train.max_lr = 1e-3\n",
    "    ARGS_BASELINE_RNN.train.epochs = 1200\n",
    "    ARGS_BASELINE_RNN.experiment.exp_name = exp_name_baseline_rnn\n",
    "\n",
    "    configurations_baseline_rnn = [\n",
    "        (\"total_trace_length\", {\n",
    "            \"model.model\": [ZeroPredictor] + [RnnKFPretrainAnalytical] * ((context_length - 1) // rnn_increment),\n",
    "            \"dataset.train.total_sequence_length\": [*range(0, context_length, rnn_increment),]\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    result_baseline_rnn = run_experiments(\n",
    "        ARGS_BASELINE_RNN, configurations_baseline_rnn, {\n",
    "            \"dir\": output_dir,\n",
    "            \"fname\": output_fname\n",
    "        }, save_experiment=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # SECTION: Save the collected experiment results\n",
    "    save = Namespace(\n",
    "        systems=systems,\n",
    "        dataset=dataset,\n",
    "        result_transformer=result_transformer,\n",
    "        result_baseline_cnn=result_baseline_cnn,\n",
    "        result_baseline_rnn=result_baseline_rnn\n",
    "    )\n",
    "    torch.save(save, save_file)\n",
    "\n",
    "\n",
    "M_transformer = get_metric_namespace_from_result(result_transformer)\n",
    "M_baseline_cnn = get_metric_namespace_from_result(result_baseline_cnn)\n",
    "M_baseline_rnn = get_metric_namespace_from_result(result_baseline_rnn)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Result processing \"\"\"\n",
    "print(\"Result processing\" + \"\\n\" + \"-\" * 120)\n",
    "systems = LinearSystemGroup(systems.values[()].td().squeeze(0), SHP.input_enabled)\n",
    "dataset = dataset.values[()].obj.squeeze(1).squeeze(0)\n",
    "\n",
    "def loss(observation_estimation: torch.Tensor) -> torch.Tensor:\n",
    "    return (dataset[\"observation\"] - observation_estimation).norm(dim=-1) ** 2\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    zero_predictor_al = utils.batch_trace(systems.S_observation_inf)\n",
    "    zero_predictor_l = loss(torch.zeros_like(dataset[\"observation\"]))\n",
    "    il = utils.batch_trace(systems.S_prediction_err_inf)\n",
    "    eil = loss(dataset[\"target\"])\n",
    "\n",
    "\n",
    "    # [n_experiments x ensemble_size x n_test_systems x test_dataset_size x context_length x O_D]\n",
    "    # -> [n_test_systems x test_dataset_size x context_length x O_D]\n",
    "    transformer_output = M_transformer.output.squeeze(1).squeeze(0)\n",
    "    # -> [n_test_systems x test_dataset_size x context_length]\n",
    "    transformer_l = loss(transformer_output)\n",
    "\n",
    "\n",
    "    # [n_firs x train.sequence_length x n_test_systems x test_dataset_size x n_experiments x ensemble_size x context_length x O_D]\n",
    "    # -> [n_firs x train.sequence_length x n_test_systems x test_dataset_size x context_length x O_D]\n",
    "    # -> [n_firs x n_test_systems x test_dataset_size x O_D x context_length]\n",
    "    # -> [n_firs x n_test_systems x test_dataset_size x context_length x O_D]\n",
    "    cnn_output = torch.diagonal(M_baseline_cnn.output.squeeze(5).squeeze(4), dim1=1, dim2=4).transpose(3, 4)\n",
    "    # -> [n_firs x n_test_systems x test_dataset_size x context_length]\n",
    "    cnn_l = loss(cnn_output)\n",
    "    # [n_firs x context_length x n_test_systems x test_dataset_size x n_experiments x ensemble_size]\n",
    "    # -> [n_firs x context_length x n_test_systems x test_dataset_size]\n",
    "    # -> [n_firs x n_test_systems x test_dataset_size x context_length]\n",
    "    cnn_al = M_baseline_cnn.al.squeeze(5).squeeze(4).permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "    # [train.sequence_length x n_test_systems x test_dataset_size x n_experiments x ensemble_size x context_length x O_D]\n",
    "    # -> [train.sequence_length x n_test_systems x test_dataset_size x context_length x O_D]\n",
    "    # -> [train.sequence_length x n_test_systems x test_dataset_size x O_D]\n",
    "    # -> [n_test_systems x test_dataset_size x train.sequence_length x O_D]\n",
    "    rnn_sequence_lengths = [*range(0, context_length, rnn_increment),]\n",
    "    rnn_output = M_baseline_rnn.output.squeeze(4).squeeze(3)[torch.arange(len(rnn_sequence_lengths)), :, :, torch.tensor(rnn_sequence_lengths)].permute(1, 2, 0, 3)\n",
    "    # [train.sequence_length x n_test_systems x test_dataset_size x n_experiments x ensemble_size]\n",
    "    # -> [train.sequence_length x n_test_systems x test_dataset_size]\n",
    "    # -> [n_test_systems x test_dataset_size x train.sequence_length]\n",
    "    rnn_al = M_baseline_rnn.al.squeeze(4).squeeze(3).permute(1, 2, 0)\n",
    "\n",
    "\n",
    "    rnn_indices = torch.tensor(rnn_sequence_lengths)\n",
    "    padded_rnn_output = torch.zeros_like(transformer_output)\n",
    "    padded_rnn_output[:, :, rnn_indices] = rnn_output\n",
    "    # -> [n_test_systems x test_dataset_size x context_length]\n",
    "    rnn_l = loss(padded_rnn_output)[:, :, rnn_indices]\n",
    "\n",
    "\n",
    "\n",
    "# SECTION: Transformer impulse response\n",
    "def cd(t: torch.Tensor) -> torch.Tensor:\n",
    "    return t.cpu().detach()\n",
    "\n",
    "def to_rgb(c: Any) -> np.ndarray:\n",
    "    return np.array(colors.to_rgb(c))\n",
    "\n",
    "\n",
    "reference_module, transformer_td = get_result_attr(result_transformer, \"learned_kfs\")[()]\n",
    "transformer_td = transformer_td.squeeze(1).squeeze(0)\n",
    "\n",
    "dataset_parameter = TensorDict.from_dict(dataset, batch_size=dataset.shape)\n",
    "dataset_parameter[\"observation\"] = nn.Parameter(dataset[\"observation\"])\n",
    "\n",
    "with torch.set_grad_enabled(True):\n",
    "    transformer_response = KF.gradient(reference_module, transformer_td, dataset_parameter, split_size=1 << 17)\n",
    "\n",
    "gradient_norm = (transformer_response[\"observation\"].norm(dim=-1) ** 2).mean(dim=1)\n",
    "for sys_idx in range(n_test_systems):\n",
    "    plt.plot(\n",
    "        cd(torch.arange(1, context_length + 1)),\n",
    "        cd(torch.flip(gradient_norm[sys_idx].clamp_min(1e-4), dims=(0,))),\n",
    "        marker=\".\", label=f\"System {sys_idx}\"\n",
    "    )\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"recency\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"gradient_norm\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# SECTION: Plotting code\n",
    "def plot(system_idx: int) -> None:\n",
    "    x = torch.arange(1, context_length + 1)\n",
    "\n",
    "    def plot_analytical(\n",
    "            l: torch.Tensor, color,\n",
    "            indices: torch.Tensor = torch.arange(context_length, dtype=torch.int),\n",
    "            error_bars: bool = False, **kwargs\n",
    "    ) -> None:\n",
    "        assert l.ndim in (2, 3), f\"Can only plot analytical loss of ndim 2 or 3 but got {l.ndim}.\"\n",
    "        assert l.shape[0] == n_test_systems, f\"First dimension of loss must match number of test systems.\"\n",
    "        plt_kwargs = {\n",
    "            \"color\": 0.6 * to_rgb(color),\n",
    "            \"linewidth\": 3,\n",
    "            \"linestyle\": \"-.\"\n",
    "        }\n",
    "        plt_kwargs.update(kwargs)\n",
    "        x_ = x[indices]\n",
    "\n",
    "        l = (l - tensordict.utils.expand_as_right(il, l))[system_idx]\n",
    "        if l.ndim == 1:\n",
    "            plt.plot(cd(x_), cd(l), **plt_kwargs, zorder=12)\n",
    "        else:\n",
    "            plt.plot(cd(x_), cd(l.median(dim=0).values), zorder=12, **plt_kwargs)\n",
    "            if error_bars:\n",
    "                plt.fill_between(\n",
    "                    cd(x_), *cd(torch.quantile(l, torch.tensor([0.25, 0.75]), dim=0)),\n",
    "                    alpha=0.1, **plt_kwargs\n",
    "                )\n",
    "\n",
    "    def plot_empirical(\n",
    "            l: torch.Tensor, name: str, color,\n",
    "            indices: torch.Tensor = torch.arange(context_length, dtype=torch.int),\n",
    "            error_bars: bool = False, **kwargs\n",
    "    ) -> None:\n",
    "        assert l.ndim == 3, f\"Empirical loss must be ndim 3 but got {l.ndim}.\"\n",
    "        assert l.shape[0] == n_test_systems, f\"First dimension of loss must match number of test systems.\"\n",
    "        plt_kwargs = {\n",
    "            \"color\": to_rgb(color),\n",
    "            \"linewidth\": 1\n",
    "        }\n",
    "        plt_kwargs.update(kwargs)\n",
    "        x_ = x[indices]\n",
    "\n",
    "        l = (l - eil[:, :, indices])[system_idx]\n",
    "        plt.plot(cd(x_), cd(l.mean(dim=0)), label=name, **plt_kwargs)\n",
    "        if error_bars:\n",
    "            plt.fill_between(\n",
    "                cd(x_), *cd(torch.quantile(l, torch.tensor([0.25, 0.75]), dim=0)),\n",
    "                alpha=0.1, **plt_kwargs\n",
    "            )\n",
    "\n",
    "    # SECTION: Plot zero predictor\n",
    "    plot_empirical(zero_predictor_l, \"zero_predictor\", \"black\")\n",
    "    plot_analytical(zero_predictor_al[:, None].expand(n_test_systems, context_length), \"black\")\n",
    "\n",
    "    # SECTION: Plot CNN baseline\n",
    "    c_list = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "    c_list[2] = \"aquamarine\"\n",
    "    for fir_length in range(3):\n",
    "        plot_empirical(cnn_l[fir_length], f\"fir_length{fir_length + 1}\", c_list[fir_length])\n",
    "        plot_analytical(cnn_al[fir_length], c_list[fir_length])\n",
    "\n",
    "    # SECTION: Plot RNN baseline\n",
    "    plot_empirical(rnn_l, \"iir\", \"crimson\", indices=rnn_indices)\n",
    "    plot_analytical(rnn_al, \"crimson\", indices=rnn_indices)\n",
    "\n",
    "    # SECTION: Plot transformer\n",
    "    plot_empirical(transformer_l, \"transformer\", \"gold\", error_bars=True, zorder=2)\n",
    "\n",
    "    plt.title(f\"InContext-GaussianA0.95/GaussianC: System {system_idx}\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"context_length\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylabel(r'normalized_loss: $|| F_\\theta(\\tau_t) - \\tau_t ||^2 - || KF(\\tau_t) - \\tau_t ||^2$')\n",
    "\n",
    "    plt.legend(framealpha=1.0)\n",
    "    plt.show()\n",
    "\n",
    "for sys_idx in range(n_test_systems):\n",
    "    plot(sys_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
