{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QupgVmcZgvyx"
   },
   "outputs": [],
   "source": [
    "## Created by Wentinn Liao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k722vkzdWafR"
   },
   "source": [
    "# Kalman Filter Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S27iMYAOuWiB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698788920879,
     "user_tz": 420,
     "elapsed": 75355,
     "user": {
      "displayName": "Wentinn Liao",
      "userId": "16122904268462266963"
     }
    },
    "outputId": "569a1105-16db-47a8-d524-6e5cf7c038b6"
   },
   "outputs": [],
   "source": [
    "#@title Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xHh9PF6hl3R",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698788920879,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Wentinn Liao",
      "userId": "16122904268462266963"
     }
    },
    "outputId": "052f664c-6ab2-4c4a-8252-be00ad928946"
   },
   "outputs": [],
   "source": [
    "#@title Symlink Setup\n",
    "import os\n",
    "\n",
    "def ptpp(PATH: str) -> str: # Converts path to python path\n",
    "    return PATH.replace('\\\\', '')\n",
    "\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/KF_RNN'\n",
    "if not os.path.exists(ptpp(DRIVE_PATH)):\n",
    "    %mkdir $DRIVE_PATH\n",
    "SYM_PATH = '/content/KF_RNN'\n",
    "if not os.path.exists(ptpp(SYM_PATH)):\n",
    "    !ln -s $DRIVE_PATH $SYM_PATH\n",
    "%cd $SYM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-h7SwLD3uWiC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698789045409,
     "user_tz": 420,
     "elapsed": 124533,
     "user": {
      "displayName": "Wentinn Liao",
      "userId": "16122904268462266963"
     }
    },
    "outputId": "8bfab9aa-6fa3-4442-a8dd-1699c0d7514d"
   },
   "outputs": [],
   "source": [
    "!pip install numpy imageio matplotlib scikit-learn torch==2.0.0 tensordict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pbdKYaruWiD"
   },
   "outputs": [],
   "source": [
    "#@title Configure Jupyter Notebook\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uK2SmUy3uWiD"
   },
   "outputs": [],
   "source": [
    "#@title Library Setup\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import math\n",
    "import functools\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import sympy as sy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "from argparse import Namespace\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fn\n",
    "import torch.utils as ptu\n",
    "import torch.optim as optim\n",
    "import torchdata\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import tensordict\n",
    "from tensordict import TensorDict\n",
    "\n",
    "from model.linear_system import LinearSystem\n",
    "from model.kf import KF\n",
    "from model.rnn_kf import RnnKF\n",
    "\n",
    "from infrastructure import utils\n",
    "from infrastructure.train import *\n",
    "\n",
    "# seed = 7\n",
    "# torch.manual_seed(seed)\n",
    "# random.seed(seed)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "dev_type = 'cuda'\n",
    "if dev_type == 'xla':\n",
    "    !pip install torch-xla cloud-tpu-client https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWpk2pvCo0Jz"
   },
   "source": [
    "# RNN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMXD3AV1nmsS"
   },
   "source": [
    "# Hyperparameter Sweep over LR and Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQ5I3Zj7slTY"
   },
   "outputs": [],
   "source": [
    "#@title Model Parameters\n",
    "ModelArgs = Namespace(\n",
    "    S_D = 6,\n",
    "    I_D = 6,\n",
    "    O_D = 4,\n",
    "    SNR = 2.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Y8dOrQZLUF8"
   },
   "outputs": [],
   "source": [
    "#@title Training Parameters\n",
    "BaseTrainArgs = Namespace(\n",
    "    # Dataset\n",
    "    train_dataset_size = 1,\n",
    "    valid_dataset_size = 100,\n",
    "    train_sequence_length = 2000,\n",
    "    valid_sequence_length = 200,\n",
    "\n",
    "    # Batch sampling\n",
    "    subsequence_length = 10,\n",
    "    subsequence_initial_mode = \"random\",    # {\"random\", \"replay_buffer\"}\n",
    "    sample_efficiency = 5,\n",
    "    replay_buffer = 10,\n",
    "    batch_size = 128,\n",
    "\n",
    "    # Optimizer\n",
    "    beta = 0.1,\n",
    "    lr = 3e-4,\n",
    "    momentum = 0.9,\n",
    "    lr_decay = 0.99,\n",
    "    optim_type = \"Adam\",                    # {\"GD\", \"SGD\", \"SGDMomentum\", \"Adam\"}\n",
    "    l2_reg = 0.1,\n",
    "\n",
    "    # Iteration\n",
    "    iterations_per_epoch = 100,\n",
    "    epochs = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTBDwFuEns1c"
   },
   "outputs": [],
   "source": [
    "#@title Experiment Parameters\n",
    "BaseExperimentArgs = Namespace(\n",
    "    n_systems = 16,\n",
    "    ensemble_size = 1,\n",
    "    log_frequency = 5,\n",
    "    print_frequency = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c-Bt_yu8m7Z"
   },
   "outputs": [],
   "source": [
    "#@title Experiment Configurations\n",
    "import itertools\n",
    "\n",
    "base_exp_name = 'SingleTrace'\n",
    "output_dir = 'sweep'\n",
    "output_fname = 'result'\n",
    "\n",
    "optim_configs = [\n",
    "    ('GD', dict()),\n",
    "    ('SGDMomentum', dict()),\n",
    "    ('Adam', {\n",
    "        'lr': 1e-2\n",
    "    })\n",
    "]\n",
    "system_configs = [\n",
    "    ('', dict()),\n",
    "    # ('ZeroObservation', {\n",
    "    #     'H': torch.zeros(ModelArgs.O_D, ModelArgs.S_D)\n",
    "    # })\n",
    "]\n",
    "\n",
    "lr_factors = torch.pow(2., torch.arange(-6., 6.5, 0.5)).tolist()\n",
    "lr_system_mode = 'different'\n",
    "momentums = [float(f'0.{t}') for t in range(10)]\n",
    "\n",
    "\n",
    "result = {}\n",
    "for (optim_config_name, optim_config), (system_config_name, system_config) in itertools.product(\n",
    "    optim_configs,\n",
    "    system_configs\n",
    "):\n",
    "    full_exp_name = f'{system_config_name}{optim_config_name}{base_exp_name}'\n",
    "    full_output_dir = f'output/{output_dir}/Full{full_exp_name}'\n",
    "    full_output_fname = f'{full_output_dir}/{output_fname}.pt'\n",
    "\n",
    "    if os.path.exists(full_output_dir):\n",
    "        with open(full_output_fname, 'rb') as fp:\n",
    "            result[optim_config_name, system_config_name] = torch.load(fp, map_location=torch.device(dev_type))\n",
    "    else:\n",
    "        to_remove = set()\n",
    "        temp_result = dict()\n",
    "        for lr_factor, momentum in itertools.product(lr_factors, momentums):\n",
    "\n",
    "            TrainArgs = copy.copy(BaseTrainArgs)\n",
    "            TrainArgs.__dict__.update(optim_config)\n",
    "            TrainArgs.optim_type = optim_config_name\n",
    "            TrainArgs.momentum = momentum\n",
    "\n",
    "            ExperimentArgs = copy.copy(BaseExperimentArgs)\n",
    "            ExperimentArgs.exp_name = full_exp_name\n",
    "\n",
    "            if optim_config_name == 'Adam':\n",
    "                TrainArgs.lr *= lr_factor\n",
    "                ExperimentArgs.exp_name += f'_lr{TrainArgs.lr}'\n",
    "            else:\n",
    "                TrainArgs.lr_factors = lr_factors\n",
    "                TrainArgs.lr_system_mode = lr_system_mode\n",
    "            if optim_config_name != 'GD':\n",
    "                ExperimentArgs.exp_name += f'_momentum{TrainArgs.momentum}'\n",
    "            ExperimentArgs.output_dir = output_dir\n",
    "\n",
    "            Args = Namespace(\n",
    "                model = ModelArgs,\n",
    "                train = TrainArgs,\n",
    "                experiment = ExperimentArgs\n",
    "            )\n",
    "\n",
    "            temp_result[optim_config_name, system_config_name, lr_factor, momentum] = run_experiment(\n",
    "                Args,\n",
    "                system_kwargs=system_config,\n",
    "                output_mode='load',\n",
    "                output_kwargs={'fname': output_fname}\n",
    "            )[0 if optim_config_name == 'Adam' else lr_factors.index(lr_factor)]\n",
    "\n",
    "            to_remove.add(f'output/{output_dir}/{ExperimentArgs.exp_name}')\n",
    "\n",
    "        result[optim_config_name, system_config_name] = torch.stack([\n",
    "            torch.stack([\n",
    "                temp_result[optim_config_name, system_config_name, lr_f, m]\n",
    "            for m in momentums])\n",
    "        for lr_f in lr_factors])\n",
    "\n",
    "        TrainArgs = copy.copy(BaseTrainArgs)\n",
    "        TrainArgs.__dict__.update(optim_config)\n",
    "        TrainArgs.optim_type = optim_config_name\n",
    "        TrainArgs.lr_factors = lr_factors\n",
    "        TrainArgs.momentum = momentums\n",
    "\n",
    "        ExperimentArgs = copy.copy(BaseExperimentArgs)\n",
    "        ExperimentArgs.exp_name = full_exp_name\n",
    "\n",
    "        Args = Namespace(\n",
    "            model = ModelArgs,\n",
    "            train = TrainArgs,\n",
    "            experiment = ExperimentArgs\n",
    "        )\n",
    "\n",
    "        %mkdir -p $full_output_dir\n",
    "        with open(f'{full_output_dir}/hparams.json', 'w') as fp:\n",
    "            json.dump(utils.toJSON(Args), fp, indent=4)\n",
    "        with open(full_output_fname, 'wb') as fp:\n",
    "            torch.save(result[optim_config_name, system_config_name], fp)\n",
    "\n",
    "        for dir in to_remove - {full_output_dir}:\n",
    "            %rm -rf $dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4U0zXpJQXCj",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698789756853,
     "user_tz": 420,
     "elapsed": 3287,
     "user": {
      "displayName": "Wentinn Liao",
      "userId": "16122904268462266963"
     }
    },
    "outputId": "dec5397f-fd9b-44a4-82de-afc3f503b5b1"
   },
   "outputs": [],
   "source": [
    "H, W = len(optim_configs), len(system_configs)\n",
    "plt.rcParams['figure.figsize'] = (12.0, 20.0)\n",
    "fig, axs = plt.subplots(H, 2 * W, subplot_kw={'projection': '3d'})\n",
    "\n",
    "tail = 10\n",
    "c = 5.\n",
    "for (h, (optim_config_name, _)), (w, (system_config_name, _)) in itertools.product(\n",
    "    enumerate(optim_configs),\n",
    "    enumerate(system_configs)\n",
    "):\n",
    "    exp_name = system_config_name + optim_config_name + base_exp_name\n",
    "    r = result[optim_config_name, system_config_name]\n",
    "\n",
    "    training_loss, overfit_loss, validation_loss, irreducible_loss = (r[k].detach().cpu() for k in (\n",
    "        'training_loss',\n",
    "        'overfit_loss',\n",
    "        'validation_loss',\n",
    "        'irreducible_loss'\n",
    "    ))\n",
    "\n",
    "    normalized_validation_loss = torch.mean(validation_loss[:, :, :, :, -tail:], dim=-1) / irreducible_loss\n",
    "    mean_normalized_validation_loss = torch.nan_to_num(torch.mean(normalized_validation_loss, dim=(2, 3)), nan=c)\n",
    "    mean_normalized_validation_loss[mean_normalized_validation_loss > 10000.] = c\n",
    "\n",
    "    lr_factors_mesh, momentums_mesh = torch.meshgrid(torch.log2(torch.DoubleTensor(lr_factors)), torch.DoubleTensor(momentums))\n",
    "\n",
    "    # View 1\n",
    "    axs[h, 2 * w].view_init(elev=15., azim=105.)\n",
    "    axs[h, 2 * w].scatter(lr_factors_mesh, momentums_mesh, mean_normalized_validation_loss, s=20, c=mean_normalized_validation_loss, cmap=plt.cm.YlOrRd_r)\n",
    "\n",
    "    axs[h, 2 * w].set_xlabel('lr_factor')\n",
    "    axs[h, 2 * w].set_ylabel('momentum')\n",
    "    axs[h, 2 * w].set_zlim(bottom=0)\n",
    "    axs[h, 2 * w].set_zlabel('validation_loss')\n",
    "    axs[h, 2 * w].set_title(exp_name)\n",
    "\n",
    "    # View 2\n",
    "    axs[h, 2 * w + 1].view_init(elev=15., azim=90.)\n",
    "    axs[h, 2 * w + 1].scatter(lr_factors_mesh, momentums_mesh, mean_normalized_validation_loss, s=20, c=mean_normalized_validation_loss, cmap=plt.cm.YlOrRd_r)\n",
    "\n",
    "    axs[h, 2 * w + 1].set_xlabel('lr_factor')\n",
    "    axs[h, 2 * w + 1].set_ylabel('momentum')\n",
    "    axs[h, 2 * w + 1].set_zlim(bottom=0)\n",
    "    axs[h, 2 * w + 1].set_zlabel('validation_loss')\n",
    "    axs[h, 2 * w + 1].set_title(exp_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-vHA1WL5uR2"
   },
   "outputs": [],
   "source": [
    "H, W = len(optim_configs), len(system_configs)\n",
    "plt.rcParams['figure.figsize'] = (12.0, 20.0)\n",
    "fig, axs = plt.subplots(H, W)\n",
    "\n",
    "for h, (optim_config_name, _) in enumerate(optim_configs):\n",
    "    for w, (system_config_name, _) in enumerate(system_configs):\n",
    "        exp_name = system_config_name + optim_config_name + base_exp_name\n",
    "\n",
    "        r = result[optim_config_name][system_config_name]\n",
    "\n",
    "        training_loss, overfit_loss, validation_loss, irreducible_loss = (r[k].detach().cpu() for k in (\n",
    "            'training_loss',\n",
    "            'overfit_loss',\n",
    "            'validation_loss',\n",
    "            'irreducible_loss'\n",
    "        ))\n",
    "\n",
    "        x = torch.arange(training_loss.shape[-1], dtype=float)\n",
    "        irreducible_loss = irreducible_loss[:, :1]\n",
    "\n",
    "        axs[h, w].plot(x, torch.ones_like(x), linestyle='--', linewidth=0.5, color='black', label='normalized irreducible_loss')\n",
    "        for lname in ('training_loss', 'overfit_loss', 'validation_loss'):\n",
    "            loss = torch.mean(eval(lname), dim=1)\n",
    "            normalized_loss = loss / irreducible_loss\n",
    "\n",
    "            mean_normalized_loss = torch.mean(normalized_loss, dim=0)\n",
    "            min_normalized_loss = torch.min(normalized_loss, dim=0)\n",
    "            max_normalized_loss = torch.max(normalized_loss, dim=0)\n",
    "\n",
    "            axs[h, w].plot(x, min_normalized_loss.values, linewidth=0.5, label=f'mean normalized {lname}')\n",
    "        #     plt.fill_between(x, min_normalized_loss, max_normalized_loss, alpha=0.2)\n",
    "\n",
    "        axs[h, w].set_xlabel('Batch')\n",
    "        axs[h, w].set_ylabel('Normalized Loss')\n",
    "        axs[h, w].set_title(exp_name)\n",
    "        # axs[h, w].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySyo5rO2k5sP"
   },
   "outputs": [],
   "source": [
    "H, W = len(optim_configs), len(system_configs)\n",
    "plt.rcParams['figure.figsize'] = (12.0, 20.0)\n",
    "fig, axs = plt.subplots(H, W)\n",
    "\n",
    "tail = 10\n",
    "cutoff = 100\n",
    "for h, (optim_config_name, _) in enumerate(optim_configs):\n",
    "    for w, (system_config_name, _) in enumerate(system_configs):\n",
    "        exp_name = system_config_name + optim_config_name + base_exp_name\n",
    "\n",
    "        r = result[optim_config_name][system_config_name]\n",
    "\n",
    "        training_loss, overfit_loss, validation_loss, irreducible_loss = (r[k].detach().cpu() for k in (\n",
    "            'training_loss',\n",
    "            'overfit_loss',\n",
    "            'validation_loss',\n",
    "            'irreducible_loss'\n",
    "        ))\n",
    "\n",
    "        x = torch.arange(cutoff, training_loss.shape[-1], dtype=float)\n",
    "\n",
    "        axs[h, w].plot(x, torch.ones_like(x), linestyle='--', linewidth=1., color='black', label='normalized irreducible_loss')\n",
    "        normalized_overfit_loss = overfit_loss / irreducible_loss[:, :, None]\n",
    "        normalized_validation_loss = validation_loss / irreducible_loss[:, :, None]\n",
    "\n",
    "        N = len(irreducible_loss)\n",
    "        for n, (overfit_loss, validation_loss) in list(enumerate(zip(normalized_overfit_loss, normalized_validation_loss)))[::2]:\n",
    "            mean_overfit_loss = torch.mean(overfit_loss, dim=0)[cutoff:]\n",
    "            mean_validation_loss = torch.mean(validation_loss, dim=0)[cutoff:]\n",
    "\n",
    "            c = color(n, scale=N)\n",
    "            axs[h, w].plot(x, mean_overfit_loss, linewidth=1., color=c, label=f'Experiment {n}')\n",
    "            axs[h, w].plot(x, mean_validation_loss, linewidth=1., color=c)\n",
    "            axs[h, w].fill_between(x, mean_overfit_loss, mean_validation_loss, alpha=0.1, color=c)\n",
    "\n",
    "        axs[h, w].set_xlabel('Batch')\n",
    "        axs[h, w].set_ylabel('Normalized Loss')\n",
    "        axs[h, w].set_title(f'{exp_name} - Normalized Overfit Loss')\n",
    "        # axs[h, w].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vp7Qps54WBoq"
   },
   "outputs": [],
   "source": [
    "H, W = len(optim_configs), len(system_configs)\n",
    "plt.rcParams['figure.figsize'] = (12.0, 20.0)\n",
    "fig, axs = plt.subplots(H, W)\n",
    "\n",
    "tail = 10\n",
    "for h, (optim_config_name, _) in enumerate(optim_configs):\n",
    "    for w, (system_config_name, _) in enumerate(system_configs):\n",
    "        exp_name = system_config_name + optim_config_name + base_exp_name\n",
    "\n",
    "        r = result[optim_config_name][system_config_name]\n",
    "\n",
    "        training_loss, overfit_loss, validation_loss, irreducible_loss = (r[k].detach().cpu() for k in (\n",
    "            'training_loss',\n",
    "            'overfit_loss',\n",
    "            'validation_loss',\n",
    "            'irreducible_loss'\n",
    "        ))\n",
    "\n",
    "        normalized_overfit_loss = overfit_loss / irreducible_loss[:, :, None]\n",
    "        normalized_validation_loss = validation_loss / irreducible_loss[:, :, None]\n",
    "\n",
    "        irreducible_, indices = torch.sort(irreducible_loss[:, 0])\n",
    "        tail_ = torch.mean(normalized_overfit_loss[:, :, -tail:], dim=-1)\n",
    "        mean_ = torch.mean(tail_, dim=1)[indices]\n",
    "        std_ = torch.std(tail_, dim=1)[indices]\n",
    "        min_ = tail_[torch.arange(len(mean_)), torch.argmin(tail_, dim=1)][indices]\n",
    "        max_ = tail_[torch.arange(len(mean_)), torch.argmax(tail_, dim=1)][indices]\n",
    "\n",
    "\n",
    "        axs_twinx = axs[h, w].twinx()\n",
    "        axs[h, w].plot(irreducible_, torch.zeros_like(mean_), linewidth=1., linestyle='--', marker='.', color='black', label=f'Mean loss')\n",
    "        axs_twinx.plot(irreducible_, mean_, linewidth=1., marker='.', color='blue', label='mean overfit loss')\n",
    "\n",
    "        axs[h, w].plot(irreducible_, min_ - mean_, linewidth=0.5, marker='.', color='turquoise')\n",
    "        # axs[h, w].plot(mean_, max_ - mean_, linewidth=0.5, marker='.', color='turquoise')\n",
    "\n",
    "        axs[h, w].fill_between(irreducible_, min_ - mean_, max_ - mean_, color='aquamarine', alpha=0.2, label='min-max')\n",
    "        axs[h, w].fill_between(irreducible_, -std_, std_, color='aquamarine', alpha=0.5, label='1 std')\n",
    "\n",
    "        axs[h, w].set_xlabel('Normalized mean loss per system')\n",
    "        axs[h, w].set_title(f'{exp_name}')\n",
    "        # axs[h, w].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zBSjk02yXHW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1lcHhNw_r_eetYfC5dd7wEm_Cgw7yytr7",
     "timestamp": 1698522777293
    },
    {
     "file_id": "1q9Qx12ZP6MjTkUXpm6XF0Tw68M9eUD0c",
     "timestamp": 1693268309398
    }
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}